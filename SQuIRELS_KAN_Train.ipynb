{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from functions import EMA, iterate_ipf, load_data\n",
    "from score_models import ReluScoreKANConv as ScoreNetworkConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA = True\n",
    "device = torch.device(\"cuda\" if CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = '_GFlash_Conv'\n",
    "\n",
    "num_steps = 20\n",
    "n = num_steps//2\n",
    "batch_size = 1024*8\n",
    "lr = 1e-5\n",
    "n_iter_glob = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_max = 0.001\n",
    "gamma_min = 0.001\n",
    "gamma_half = np.linspace(gamma_min, gamma_max, n)\n",
    "gammas = np.concatenate([gamma_half, np.flip(gamma_half)])\n",
    "gammas = torch.tensor(gammas).to(device)\n",
    "T = torch.sum(gammas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_layers=[16,16]\n",
    "# temb_dim=8\n",
    "# conv_dof=2\n",
    "\n",
    "normalize_energy = False\n",
    "# model_version = f\"_{encoder_layers[0]}_{temb_dim}_{conv_dof}_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abs_path = '/mnt/d/UFRGS/TCC/Dados/'\n",
    "abs_path = '/media/marcelomd/HDD2/UFRGS/TCC/Dados/'\n",
    "data_dir_path = abs_path + 'datasets/SB_Refinement/'\n",
    "models_dir_path = abs_path + 'repos/sb_ref_kan/models/'\n",
    "\n",
    "file_path_gflash = data_dir_path + 'run_GFlash01_100k_10_100GeV_full.npy'\n",
    "file_path_g4 = data_dir_path + 'run_Geant_100k_10_100GeV_full.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(file_path_gflash, file_path_g4, normalize_energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_gflash = data[\"energy_gflash\"]\n",
    "energy_particle_gflash = data[\"energy_particle_gflash\"]\n",
    "energy_voxel_gflash = data[\"energy_voxel_gflash\"]\n",
    "energy_g4 = data[\"energy_g4\"]\n",
    "energy_particle_g4 = data[\"energy_particle_g4\"]\n",
    "energy_voxel_g4 = data[\"energy_voxel_g4\"]\n",
    "\n",
    "npar = int(energy_voxel_g4.shape[0])\n",
    "            \n",
    "X_init = energy_voxel_gflash\n",
    "Y_init = np.concatenate((energy_gflash, energy_g4, energy_particle_gflash), 1)\n",
    "init_sample = torch.tensor(X_init).view(X_init.shape[0], 1, 10, 10)\n",
    "init_lable = torch.tensor(Y_init)\n",
    "scaling_factor = 7\n",
    "#init_sample = (init_sample - init_sample.mean()) / init_sample.std() * scaling_factor\n",
    "init_ds = TensorDataset(init_sample, init_lable)\n",
    "init_dl = DataLoader(init_ds, batch_size=batch_size, shuffle=False)\n",
    "#init_dl = repeater(init_dl)\n",
    "# print(init_sample.shape)\n",
    "\n",
    "X_final = energy_voxel_g4\n",
    "Y_final = np.concatenate((energy_g4, energy_gflash, energy_particle_g4), 1)\n",
    "scaling_factor = 7.\n",
    "final_sample = torch.tensor(X_final).view(X_final.shape[0], 1, 10, 10)\n",
    "final_label = torch.tensor(Y_final)\n",
    "#final_sample = (final_sample - final_sample.mean()) / final_sample.std() * scaling_factor\n",
    "final_ds = TensorDataset(final_sample, final_label)\n",
    "final_dl = DataLoader(final_ds, batch_size=batch_size, shuffle=False)\n",
    "#final_dl = repeater(final_dl)\n",
    "\n",
    "#mean_final = torch.tensor(0.)\n",
    "#var_final = torch.tensor(1.*10**3) #infty like\n",
    "\n",
    "mean_final = torch.zeros(1, 10, 10).to(device)\n",
    "var_final = 1.*torch.ones(1, 10, 10).to(device)\n",
    "\n",
    "# print(final_sample.shape)\n",
    "# print(mean_final.shape)\n",
    "# print(var_final.shape)\n",
    "\n",
    "\n",
    "dls = {'f': init_dl, 'b': final_dl}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from score_models import BottleneckScoreKAGNConv as ScoreNetworkConv\n",
    "\n",
    "i = 16\n",
    "encoder_layers=[i,i]\n",
    "temb_dim=8\n",
    "conv_dof=2\n",
    "\n",
    "model_f = ScoreNetworkConv(encoder_layers=encoder_layers,\n",
    "                           temb_dim=temb_dim,\n",
    "                           conv_dof=conv_dof,\n",
    "                           n_cond = init_lable.size(1)).to(device)\n",
    "\n",
    "model_version = f\"_{encoder_layers[0]}_{temb_dim}_{conv_dof}_\"\n",
    "\n",
    "sum(p.numel() for p in model_f.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_f = ScoreNetworkConv(encoder_layers=encoder_layers,\n",
    "                           temb_dim=temb_dim,\n",
    "                           conv_dof=conv_dof,\n",
    "                           n_cond = init_lable.size(1)).to(device)\n",
    "\n",
    "model_b = ScoreNetworkConv(encoder_layers=encoder_layers,\n",
    "                           temb_dim=temb_dim,\n",
    "                           conv_dof=conv_dof,\n",
    "                           n_cond = init_lable.size(1)).to(device)\n",
    "\n",
    "model_name = str(model_f.__class__)[21:-2]\n",
    "\n",
    "model_f = torch.nn.DataParallel(model_f)\n",
    "model_b = torch.nn.DataParallel(model_b)\n",
    "\n",
    "opt_f = torch.optim.Adam(model_f.parameters(), lr=lr)\n",
    "opt_b = torch.optim.Adam(model_b.parameters(), lr=lr)\n",
    "\n",
    "net_f = EMA(model=model_f, decay=0.95).to(device)\n",
    "net_b = EMA(model=model_b, decay=0.95).to(device)\n",
    "\n",
    "nets  = {'f': net_f, 'b': net_b, 'iter_loss': [], 'iter_et': [] }\n",
    "opts  = {'f': opt_f, 'b': opt_b }\n",
    "\n",
    "nets['f'].train()\n",
    "nets['b'].train()\n",
    "\n",
    "\n",
    "d = init_sample[0].shape  # shape of object to diffuse\n",
    "dy = init_lable[0].shape  # shape of object to diffuse\n",
    "print(d)\n",
    "print(dy)\n",
    "\n",
    "#print(net_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "f = open(models_dir_path + model_name + model_version + \".txt\", 'w', encoding=\"utf-8\")\n",
    "f.write(\"loss;elapsed time;iteration\\n\")\n",
    "\n",
    "start_iter=0\n",
    "\n",
    "for i in range(1, 400):\n",
    "    try:\n",
    "        nets['f'].load_state_dict(torch.load(models_dir_path + 'Iter{:d}_net_f'.format(i) + suffix + model_name + model_version + '.pth', map_location=device))\n",
    "        nets['b'].load_state_dict(torch.load(models_dir_path + 'Iter{:d}_net_b'.format(i) + suffix + model_name + model_version + '.pth', map_location=device))\n",
    "        \n",
    "        start_iter = i\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "if start_iter == 0:\n",
    "    iterate_ipf(nets=nets, opts=opts, device=device, dls=dls, gammas=gammas, npar=npar, batch_size=batch_size,\n",
    "                num_steps=num_steps, d=d, dy=dy, T=T, mean_final=mean_final, var_final=var_final, n_iter=100,\n",
    "                forward_or_backward='f', forward_or_backward_rev='b', first=True)\n",
    "    for l, t in zip(nets['iter_loss'],nets['iter_et']):\n",
    "        f.write(f\"{l:.6f};{t:.2f};0\\n\")\n",
    "    print('--------------- Done iter 0 ---------------')\n",
    "    \n",
    "nets['f'].train()\n",
    "nets['b'].train()\n",
    "\n",
    "for i in range(start_iter+1, start_iter+20):\n",
    "\n",
    "    iterate_ipf(nets=nets, opts=opts, device=device, dls=dls, gammas=gammas, npar=npar, batch_size=batch_size,\n",
    "                num_steps=num_steps, d=d, dy=dy, T=T, mean_final=mean_final, var_final=var_final, n_iter=n_iter_glob,\n",
    "                forward_or_backward='b', forward_or_backward_rev='f', first=False)\n",
    "    for l, t in zip(nets['iter_loss'],nets['iter_et']):\n",
    "        f.write(f\"{l:.6f};{t:.2f};{i}\\n\")\n",
    "    print('--------------- Done iter B{:d} ---------------'.format(i))\n",
    "\n",
    "    iterate_ipf(nets=nets, opts=opts, device=device, dls=dls, gammas=gammas, npar=npar, batch_size=batch_size,\n",
    "                num_steps=num_steps, d=d, dy=dy, T=T, mean_final=mean_final, var_final=var_final, n_iter=n_iter_glob,\n",
    "                forward_or_backward='f', forward_or_backward_rev='b', first=False)\n",
    "    for l, t in zip(nets['iter_loss'],nets['iter_et']):\n",
    "        f.write(f\"{l:.6f};{t:.2f};{i}\\n\")\n",
    "    print('--------------- Done iter F{:d} ---------------'.format(i))\n",
    "\n",
    "    torch.save(net_f.state_dict(), models_dir_path + 'Iter{:d}_net_f'.format(i) + suffix + model_name + model_version + '.pth')\n",
    "    torch.save(net_b.state_dict(), models_dir_path + 'Iter{:d}_net_b'.format(i) + suffix + model_name + model_version + '.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sb_ref",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
